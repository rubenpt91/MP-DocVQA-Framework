{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jordy/miniconda3/envs/mp_docvqa/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['</s>', '<unk>', '<pad>', '<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jordy/miniconda3/envs/mp_docvqa/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "print(tokenizer.all_special_tokens)\n",
    "\n",
    "#tokenizer.add_tokens(['<some_token_1>', '<some_token_2'>])\n",
    "\n",
    "#model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "#model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples of answers of different types and how we would encode them\n",
    "\n",
    "single = [\"TRIPLE PLAY CONSULTING, Inc.,\"]\n",
    "listtype = [\n",
    "            \"Board of Education\",\n",
    "            \"City of Chicago\"\n",
    "        ]\n",
    "nonetype = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing tokenization for single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 16840, 27872, 17501, 476, 8472, 134, 4254, 21089, 6, 1542, 5, 6, 1]\n",
      "TRIPLE PLAY CONSULTING, Inc.,</s>\n",
      "TRIPLE PLAY CONSULTING, Inc.,\n",
      "----\n",
      "[2086, 13, 2855, 3, 184, 184, 896, 13, 3715, 1]\n",
      "Board of Education && City of Chicago</s>\n",
      "Board of Education && City of Chicago\n",
      "----\n",
      "[2086, 13, 2855, 1820, 896, 13, 3715, 1]\n",
      "Board of Education | City of Chicago</s>\n",
      "Board of Education | City of Chicago\n",
      "----\n",
      "[1]\n",
      "</s>\n",
      "\n",
      "----\n",
      "[1]\n",
      "</s>\n",
      "\n",
      "----\n",
      "[5839, 1]\n",
      "none</s>\n",
      "none\n",
      "----\n",
      "before\n",
      "[3, 2, 567, 7894, 3155, 1]\n",
      "<unk> NONE></s>\n",
      "⁇ NONE>\n",
      "after\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<NONE>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "def nonetype_specialtoken(answers:List,tokenizer, value=\"<NONE>\"):\n",
    "    tokenizer.add_tokens([value])\n",
    "    return value\n",
    "\n",
    "def nonetype_empty(answers:List, value=\"\"):\n",
    "    return value\n",
    "\n",
    "def listtype_sep(answers:List, separator=\" | \"):\n",
    "    return separator.join(answers)\n",
    "\n",
    "def check_tokenization(answer:str):\n",
    "    tokenized = tokenizer(answer).input_ids\n",
    "    decoded = tokenizer.decode(tokenized)\n",
    "    converted = tokenizer.convert_tokens_to_string(tokenized)\n",
    "\n",
    "    print(\"\\n\".join([str(tokenized), decoded, converted]))\n",
    "\n",
    "\n",
    "check_tokenization(single[0])\n",
    "print(\"----\")\n",
    "check_tokenization(listtype_sep(listtype, separator=\" && \"))\n",
    "print(\"----\")\n",
    "check_tokenization(listtype_sep(listtype))\n",
    "print(\"----\")\n",
    "check_tokenization(nonetype_empty(nonetype))\n",
    "print(\"----\")\n",
    "check_tokenization(nonetype_empty(nonetype, value=\" \"))\n",
    "print(\"----\")\n",
    "check_tokenization(nonetype_empty(nonetype, value=\"none\"))\n",
    "print(\"----\")\n",
    "print(\"before\")\n",
    "check_tokenization(\"<NONE>\")\n",
    "print(\"after\")\n",
    "nonetype_specialtoken(nonetype,tokenizer, value=\"<NONE>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before\n",
      "[10144, 1]\n",
      "NA</s>\n",
      "NA\n",
      "after\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NA'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"before\")\n",
    "check_tokenization(\"NA\")\n",
    "print(\"after\")\n",
    "nonetype_specialtoken(nonetype,tokenizer, value=\"NA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n",
    "labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "# the forward function automatically creates the correct decoder_input_ids\n",
    "loss = model(input_ids=input_ids, labels=labels).loss\n",
    "loss.item()\n",
    "\n",
    "#additional_special_tokens\n",
    "\n",
    "#tokenizer.build_inputs_with_special_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check how many tokens answers have on average\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DescribeResult(nobs=6324, minmax=(1, 20), mean=4.609266287160025, variance=14.162817807302295, skewness=2.2471201480632748, kurtosis=5.239790552256563)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "FILEPATH = \"../save/results/T5_DUDE_concat__2023-02-23_17-25-38.json\"\n",
    "results = json.load(open(FILEPATH, \"r\"))\n",
    "\n",
    "answers = [x['pred_answer'] for k, x in results[\"Scores by samples\"].items()]\n",
    "n_tokens = [len(tokenizer(a).input_ids) for a in answers]\n",
    "from scipy.stats import describe\n",
    "describe(n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (565 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mtokenized\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m\"\u001b[39m\u001b[39mA\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m a: tokenizer(a)\u001b[39m.\u001b[39minput_ids)\n\u001b[1;32m     11\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mn_tokens\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m\"\u001b[39m\u001b[39mA\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m a: \u001b[39mlen\u001b[39m(tokenizer(a)\u001b[39m.\u001b[39minput_ids))\n\u001b[0;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m \u001b[39mimport\u001b[39;00m describe\n\u001b[1;32m     14\u001b[0m describe(df[\u001b[39m\"\u001b[39m\u001b[39mn_tokens\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scipy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "FILEPATH = \"./trainval_QA-pairs.csv\"\n",
    "df = pd.read_csv(FILEPATH)\n",
    "df[\"A\"] = df[\"A\"].apply(lambda a: \"none\" if a == \"[]\" else str(a))\n",
    "\n",
    "df[\"tokenized\"] = df[\"A\"].apply(lambda a: tokenizer(a).input_ids)\n",
    "df[\"n_tokens\"] = df[\"A\"].apply(lambda a: len(tokenizer(a).input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the 5 disruptive technologies in 2013, according to IBM?\n",
      "what is the client recive mmethods in the document?\n",
      "What is the statements of significant?\n",
      "How much classroom time is required for a bombardier to become familiar with the bombsight?\n",
      "How are bounding boxes represented?\n",
      "What about coverage for behavioral health service?\n",
      "Which 2 groups did The Honourable Jody Wilson-Raybound that a formal inquiry be conducted in regards to Dr. Hassan Diab's affair?\n",
      "How many reforms were proposed in the document?\n",
      "Describe the faculty requirements summary?\n",
      "How many people are involved in supporting the International Space Station (ISS)?\n",
      "WHAT KIND OF RED APPLE ENTITY?\n",
      "What are the plaintiffs accusing the defendants of?\n",
      "What are the implications of Title 18 Sections 793 and 794, of the U.S. Code as amended?\n",
      "What is the time and the kind of services offered by the scientists at the First church of Christ?\n",
      "What is INFLATION KIT?\n",
      "Is there pink lines ?\n",
      "How climate change can affect pregnant woman?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import describe\n",
    "describe(df[\"n_tokens\"])\n",
    "\n",
    "print(\"\\n\".join([df[\"Q\"].iloc[i] for i, x in enumerate(df[\"n_tokens\"]) if x > 100]))\n",
    "df.plot.box(y=\"n_tokens\")#.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "269.43600000000515"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "stats.scoreatpercentile(df['n_tokens'], 99.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test token addition and initialization strategies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jordy/miniconda3/envs/mp_docvqa/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/jordy/miniconda3/envs/mp_docvqa/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32100\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "print(len(tokenizer.get_vocab()))\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ -0.7539,   0.5977,  -2.4375,  ...,   1.2500,  -0.7891,   3.5156],\n",
       "        [ 11.3750,  -4.8750,   9.0625,  ...,   4.8438,  14.3750,  -5.7812],\n",
       "        [-16.6250,  11.0625, -20.8750,  ...,  10.6875,  22.2500,  25.0000],\n",
       "        ...,\n",
       "        [  2.2344,   6.7500, -11.0625,  ..., -11.3125,  13.5625,  16.6250],\n",
       "        [  4.2500,   5.1250, -12.2500,  ..., -11.9375,  13.5000,  17.0000],\n",
       "        [  4.0625,   6.9688, -12.2500,  ..., -11.3750,  11.9375,  16.6250]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.encoder.embeddings.word_embeddings.weight\n",
    "model.shared.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32105\n",
      "32105\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def initialize_tokens_by_averaging(tokenizer, model, sorted_tokens):\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, len(sorted_tokens)):\n",
    "\n",
    "            tokens = tokenizer.tokenize(sorted_tokens[idx])\n",
    "\n",
    "            tokenized_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "            tokenizer.add_tokens(sorted_tokens[idx])\n",
    "\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "            ##model.bert.embeddings.word_embeddings.weight\n",
    "            #specific to T5\n",
    "\n",
    "def initialize_tokens_randomly(tokenizer, model, sorted_tokens):\n",
    "\n",
    "    for idx in range(0, len(sorted_tokens)):\n",
    "        tokenizer.add_tokens(sorted_tokens[idx])\n",
    "\n",
    "    # resize embedding layers\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "\n",
    "QTYPES = [\"extractive\", \"abstractive\", \"list/abstractive\", \"list/extractive\", \"not-answerable\"]\n",
    "\n",
    "initialize_tokens_randomly(tokenizer, model, QTYPES)\n",
    "print(len(tokenizer.get_vocab()))\n",
    "\n",
    "initialize_tokens_by_averaging(tokenizer, model, QTYPES)\n",
    "print(len(tokenizer.get_vocab()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confidence estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    decoder_input_ids=prompt_tensors,\n",
    "    encoder_outputs=encoder_outputs,\n",
    "\n",
    "gen_sequences = decoder_output.sequences[:, prompt_tensors.shape[-1]:-1]\n",
    "probs = torch.stack(decoder_output.scores, dim=1).softmax(-1)\n",
    "gen_probs = torch.gather(probs, 2, gen_sequences[:, :, None]).squeeze(-1)\n",
    "unique_prob_per_sequence = gen_probs.prod(-1)\n",
    "\n",
    "def compute_logprob(model, input_dict: Dict[str, torch.Tensor]):  # first token is omitted\n",
    "  labels = input_dict['input_ids']\n",
    "  outputs = model(**input_dict, labels=labels)\n",
    "  logits = outputs[1].detach()\n",
    "  logprobs = F.log_softmax(logits, dim=-1)\n",
    "  #logprobs = torch.gather(logprobs, -1, input_dict['input_ids'].unsqueeze(-1)).squeeze(-1)\n",
    "  logprobs = torch.gather(logprobs[:, :-1, :], -1, labels[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "  first_lps = torch.zeros_like(logprobs[:, :1])  # assume the first token always have prob of 1\n",
    "  logprobs = torch.cat([first_lps, logprobs], 1)\n",
    "  return logprobs\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test atype and qtype tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.7.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting seaborn\n",
      "  Downloading seaborn-0.12.2-py3-none-any.whl (293 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.3/293.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /home/jordy/miniconda3/envs/mp_docvqa/lib/python3.9/site-packages (from matplotlib) (1.24.2)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.4-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.0.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.7/299.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.4/965.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /home/jordy/miniconda3/envs/mp_docvqa/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Collecting importlib-resources>=3.2.0\n",
      "  Downloading importlib_resources-5.12.0-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/jordy/miniconda3/envs/mp_docvqa/lib/python3.9/site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jordy/miniconda3/envs/mp_docvqa/lib/python3.9/site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: pandas>=0.25 in /home/jordy/miniconda3/envs/mp_docvqa/lib/python3.9/site-packages (from seaborn) (1.5.3)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/jordy/miniconda3/envs/mp_docvqa/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.14.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/jordy/miniconda3/envs/mp_docvqa/lib/python3.9/site-packages (from pandas>=0.25->seaborn) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/jordy/miniconda3/envs/mp_docvqa/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Installing collected packages: pyparsing, kiwisolver, importlib-resources, fonttools, cycler, contourpy, matplotlib, seaborn\n",
      "Successfully installed contourpy-1.0.7 cycler-0.11.0 fonttools-4.38.0 importlib-resources-5.12.0 kiwisolver-1.4.4 matplotlib-3.7.0 pyparsing-3.0.9 seaborn-0.12.2\n"
     ]
    }
   ],
   "source": [
    "!pip3 install matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jordy/.virtualenvs/SOTA/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    " \n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n",
    "labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "# the forward function automatically creates the correct decoder_input_ids\n",
    "output = model(input_ids=input_ids, labels=labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jordy/.virtualenvs/SOTA/lib/python3.8/site-packages/transformers/generation/utils.py:1387: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.62463284]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "input_ids = tokenizer(\"translate English to German: The house is wonderful.\", return_tensors=\"pt\").input_ids\n",
    "output = model.generate(**tokenizer(\"translate English to German: The house is wonderful.\", return_tensors=\"pt\"), output_scores=True,\n",
    "            return_dict_in_generate=True,\n",
    "            output_attentions=True)\n",
    "pred_answers = tokenizer.batch_decode(\n",
    "    output[\"sequences\"], skip_special_tokens=True\n",
    ")\n",
    "bs=1\n",
    "all_logits = torch.stack(output.scores)\n",
    "token_logits = np.zeros(len(output[\"scores\"][0]))\n",
    "for seq_ix in range(len(output[\"scores\"])):\n",
    "    for batch_ix in range(bs):\n",
    "        token_id = output.sequences[batch_ix, seq_ix + 1]\n",
    "        best_logits[batch_ix] += (\n",
    "            all_logits[seq_ix, batch_ix, token_id]\n",
    "            if token_id not in tokenizer.all_special_ids\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "print(best_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-34-18a8a62a5e58>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-34-18a8a62a5e58>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    shape ut.scores[0])\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-f51fa4bf101b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_ix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtoken_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_ix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_ix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mspecial_token_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_ix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_ix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_id\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspecial_token_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "special_token_mask= np.zeros((len(output[\"scores\"][0]),bs),dtype=bool) #scores skips first\n",
    "bs \n",
    "\n",
    "for seq_ix in range(len(output[\"scores\"])):\n",
    "    for batch_ix in range(bs):\n",
    "        token_id = output.sequences[batch_ix, seq_ix + 1]\n",
    "        special_token_mask[seq_ix,batch_ix]=bool(token_id not in tokenizer.all_special_ids) \n",
    "print(special_token_mask)    \n",
    "  \n",
    "decoder_output_confs = torch.amax(torch.stack(output.scores, dim=1).softmax(-1), 2).cpu().numpy() \n",
    "print(decoder_output_confs[special_token_mask]) \n",
    " \n",
    "# add score for end token and wrap scores in a list\n",
    "decoder_output_confs = [np.concatenate([decoder_output_confs, [1.]], axis=0)]\n",
    "print(output.sequences[0],pred_answers,decoder_output_confs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-f5cfb655a4c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "tokenizer.get_vocab()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], size=(1, 0), dtype=torch.int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_sequences = output.sequences[:, input_ids.shape[-1]:-1]\n",
    "gen_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<extra_id_0> park<extra_id_1><extra_id_1> the<extra_id_2>dog'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# gen_sequences = decoder_output.sequences[:, prompt_tensors.shape[-1]:-1]\n",
    "# probs = torch.stack(decoder_output.scores, dim=1).softmax(-1)\n",
    "# gen_probs = torch.gather(probs, 2, gen_sequences[:, :, None]).squeeze(-1)\n",
    "# unique_prob_per_sequence = gen_probs.prod(-1)\n",
    "path=logits[0].argmax(-1)\n",
    "tokenizer.decode(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## only use id's that were generated\n",
    "gen_sequences = generated_outputs.sequences[:, inputs.input_ids.shape[-1]:]\n",
    "probs = torch.stack(generated_outputs.scores, dim=1).softmax(-1)  # -> shape [3, seqlen, vocab_size]\n",
    "\n",
    "# now we need to collect the probability of the generated token\n",
    "# we need to add a dummy dim in the end to make gather work\n",
    "gen_probs = torch.gather(probs, 2, gen_sequences[:, :, None]).squeeze(-1)\n",
    "\n",
    "# 1) the probs that exactly those sequences are generated again\n",
    "# those are normally going to be very small\n",
    "unique_prob_per_sequence = gen_probs.prod(-1)\n",
    "\n",
    "# 2) normalize the probs over the three sequences\n",
    "# WEIRD partition function\n",
    "normed_gen_probs = gen_probs / gen_probs.sum(0)\n",
    "assert normed_gen_probs[:, 0].sum() == 1.0, \"probs should be normalized\"\n",
    "\n",
    "# 3) compare normalized probs to each other like in 1)\n",
    "unique_normed_prob_per_sequence = normed_gen_probs.prod(-1)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(generated_outputs.sequences):\n",
    "    print(\"{}: {}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True), unique_normed_prob_per_sequence[i]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "9e12bd35d6e12f8f07b4099e0c6a8e539513db727fbd371a4b61ac6721d7a75a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
