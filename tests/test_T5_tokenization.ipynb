{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['</s>', '<unk>', '<pad>', '<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jordy/miniconda3/envs/mp_docvqa/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "print(tokenizer.all_special_tokens)\n",
    "\n",
    "#tokenizer.add_tokens(['<some_token_1>', '<some_token_2'>])\n",
    "\n",
    "#model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "#model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples of answers of different types and how we would encode them\n",
    "\n",
    "single = [\"TRIPLE PLAY CONSULTING, Inc.,\"]\n",
    "listtype = [\n",
    "            \"Board of Education\",\n",
    "            \"City of Chicago\"\n",
    "        ]\n",
    "nonetype = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing tokenization for single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 16840, 27872, 17501, 476, 8472, 134, 4254, 21089, 6, 1542, 5, 6, 1]\n",
      "TRIPLE PLAY CONSULTING, Inc.,</s>\n",
      "TRIPLE PLAY CONSULTING, Inc.,\n",
      "----\n",
      "[2086, 13, 2855, 3, 184, 184, 896, 13, 3715, 1]\n",
      "Board of Education && City of Chicago</s>\n",
      "Board of Education && City of Chicago\n",
      "----\n",
      "[2086, 13, 2855, 1820, 896, 13, 3715, 1]\n",
      "Board of Education | City of Chicago</s>\n",
      "Board of Education | City of Chicago\n",
      "----\n",
      "[1]\n",
      "</s>\n",
      "\n",
      "----\n",
      "[1]\n",
      "</s>\n",
      "\n",
      "----\n",
      "[5839, 1]\n",
      "none</s>\n",
      "none\n",
      "----\n",
      "before\n",
      "[3, 2, 567, 7894, 3155, 1]\n",
      "<unk> NONE></s>\n",
      "⁇ NONE>\n",
      "after\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<NONE>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "def nonetype_specialtoken(answers:List,tokenizer, value=\"<NONE>\"):\n",
    "    tokenizer.add_tokens([value])\n",
    "    return value\n",
    "\n",
    "def nonetype_empty(answers:List, value=\"\"):\n",
    "    return value\n",
    "\n",
    "def listtype_sep(answers:List, separator=\" | \"):\n",
    "    return separator.join(answers)\n",
    "\n",
    "def check_tokenization(answer:str):\n",
    "    tokenized = tokenizer(answer).input_ids\n",
    "    decoded = tokenizer.decode(tokenized)\n",
    "    converted = tokenizer.convert_tokens_to_string(tokenized)\n",
    "\n",
    "    print(\"\\n\".join([str(tokenized), decoded, converted]))\n",
    "\n",
    "\n",
    "check_tokenization(single[0])\n",
    "print(\"----\")\n",
    "check_tokenization(listtype_sep(listtype, separator=\" && \"))\n",
    "print(\"----\")\n",
    "check_tokenization(listtype_sep(listtype))\n",
    "print(\"----\")\n",
    "check_tokenization(nonetype_empty(nonetype))\n",
    "print(\"----\")\n",
    "check_tokenization(nonetype_empty(nonetype, value=\" \"))\n",
    "print(\"----\")\n",
    "check_tokenization(nonetype_empty(nonetype, value=\"none\"))\n",
    "print(\"----\")\n",
    "print(\"before\")\n",
    "check_tokenization(\"<NONE>\")\n",
    "print(\"after\")\n",
    "nonetype_specialtoken(nonetype,tokenizer, value=\"<NONE>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before\n",
      "[10144, 1]\n",
      "NA</s>\n",
      "NA\n",
      "after\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NA'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"before\")\n",
    "check_tokenization(\"NA\")\n",
    "print(\"after\")\n",
    "nonetype_specialtoken(nonetype,tokenizer, value=\"NA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n",
    "labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "# the forward function automatically creates the correct decoder_input_ids\n",
    "loss = model(input_ids=input_ids, labels=labels).loss\n",
    "loss.item()\n",
    "\n",
    "#additional_special_tokens\n",
    "\n",
    "#tokenizer.build_inputs_with_special_tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test atype and qtype tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mp_docvqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9e12bd35d6e12f8f07b4099e0c6a8e539513db727fbd371a4b61ac6721d7a75a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
